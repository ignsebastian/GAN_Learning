{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_Try.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3qfJ8ySSywh0bFZWCRnAt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ignsebastian/GAN_Learning/blob/main/GAN_Try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UpE0Fz6Fkeu",
        "outputId": "8acefd08-75f3-4b36-e7d9-6e862a64f191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "'2.3.0'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kBhlFt9FrrC",
        "outputId": "e4e07f07-3506-4aef-af30-adb5199522c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# To generate GIFs\n",
        "!pip install -q imageio\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUp2XYJ8FtNO"
      },
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkG4ZnZuFu-D",
        "outputId": "ce9bdab4-040d-47de-af11-cd31267964b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBr51DnXFwVm"
      },
      "source": [
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms0bRl0LFxxg"
      },
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8ewHiOBFz6B"
      },
      "source": [
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4EFKC4lF4M1"
      },
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS-4PK4BGiIf",
        "outputId": "1926479c-2f62-40e0-ea0d-f3cf225667fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7d6656d860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpHXYQoWGnI3"
      },
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[28, 28, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9Z96-YjGo0e",
        "outputId": "44112328-d388-40ed-b885-fefa2e056a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.00105763]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvBii4WGHWIA"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHl_zxeoHXY0"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRSdLNa3HX1z"
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64_NUX9_Haaf"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovF7o9qmHcdc"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJIRoIQ7HenW"
      },
      "source": [
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxIXSab4Hga9"
      },
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nna0OAtUHlKi"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as we go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ2Dy1p0Hmta"
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "levU-CIxHorK"
      },
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZYHMwQAHqBQ"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05kxNjWIH_vu"
      },
      "source": [
        "# Display a single image using the epoch number\n",
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHmb7eh3IBDQ"
      },
      "source": [
        "display_image(EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sYgWBA2ICI0"
      },
      "source": [
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-qCwu6nIDsE"
      },
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(anim_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}